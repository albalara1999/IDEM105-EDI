{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4906eb7b-91bf-4763-b4ff-5847f4c81de6",
   "metadata": {},
   "source": [
    "# Computational Text Analysis Fundamentals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048abf89-a2a3-4a73-9a0a-464b6c83f331",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenization refers to the process of dividing a text into \"tokens\": words, parts of words, phrases, punctuations, or even sentences. The most common tokenization is at the level of words. A common strategy is to \"split\" the text wherever one encounters spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35523489-1931-4c28-82d2-36e99851792c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"\"\"\n",
    "Well the design has developed a wee bit since you saw it last time, \n",
    "the design obviously is still in exactly the same place but \n",
    "the design is extended to actually include the actual cremator facility,\n",
    "so if I can start with this particular drawing, you’ve seen a version\n",
    "of this drawing before. Basically we’re arriving in the new car park\n",
    "in this area and from the car park we’ll enter the building through a\n",
    "waiting area. This leads us to the first query I have because there was\n",
    "some discussion about whether you wanted the size of the waiting room\n",
    "increased. At the moment it’s exactly on brief, but it does look kind of\n",
    "small to my eye in relation to the size of the project.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af15935-9b9f-4ec6-a73a-5d7c82f11784",
   "metadata": {},
   "source": [
    "### Simple approach: split at the spaces\n",
    "A common strategy is to \"split\" the text wherever one encounters spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf3d70ce-4e3e-414a-877a-1e3ee7462c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Well', 'the', 'design', 'has', 'developed', 'a', 'wee', 'bit', 'since', 'you', 'saw', 'it', 'last', 'time,', 'the', 'design', 'obviously', 'is', 'still', 'in', 'exactly', 'the', 'same', 'place', 'but', 'the', 'design', 'is', 'extended', 'to', 'actually', 'include', 'the', 'actual', 'cremator', 'facility,', 'so', 'if', 'I', 'can', 'start', 'with', 'this', 'particular', 'drawing,', 'you’ve', 'seen', 'a', 'version', 'of', 'this', 'drawing', 'before.', 'Basically', 'we’re', 'arriving', 'in', 'the', 'new', 'car', 'park', 'in', 'this', 'area', 'and', 'from', 'the', 'car', 'park', 'we’ll', 'enter', 'the', 'building', 'through', 'a', 'waiting', 'area.', 'This', 'leads', 'us', 'to', 'the', 'first', 'query', 'I', 'have', 'because', 'there', 'was', 'some', 'discussion', 'about', 'whether', 'you', 'wanted', 'the', 'size', 'of', 'the', 'waiting', 'room', 'increased.', 'At', 'the', 'moment', 'it’s', 'exactly', 'on', 'brief,', 'but', 'it', 'does', 'look', 'kind', 'of', 'small', 'to', 'my', 'eye', 'in', 'relation', 'to', 'the', 'size', 'of', 'the', 'project.']\n"
     ]
    }
   ],
   "source": [
    "tokens = sample_text.split()\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acf8c859-e96c-41b1-b140-a1f0326fee8d",
   "metadata": {},
   "source": [
    "### Preferred Approach: Use an existing library.\n",
    "Note the punctuations in the above output. They are still part of the preceding word. \n",
    "Also note contractions like `you've`. They are retained as they are. \n",
    "There are different ways to separate punctuations, contractions etc., but thankfully we can use a pre-existing library called [Natural Language Toolkit or NLTK](https://www.nltk.org/index.html#). To generate tokens, we will use a function called [word_tokenize](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.word_tokenize)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d0ce9434-2bbf-4a9d-aa9e-6576f7de3ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Well', 'the', 'design', 'has', 'developed', 'a', 'wee', 'bit', 'since', 'you', 'saw', 'it', 'last', 'time', ',', 'the', 'design', 'obviously', 'is', 'still', 'in', 'exactly', 'the', 'same', 'place', 'but', 'the', 'design', 'is', 'extended', 'to', 'actually', 'include', 'the', 'actual', 'cremator', 'facility', ',', 'so', 'if', 'I', 'can', 'start', 'with', 'this', 'particular', 'drawing', ',', 'you', '’', 've', 'seen', 'a', 'version', 'of', 'this', 'drawing', 'before', '.', 'Basically', 'we', '’', 're', 'arriving', 'in', 'the', 'new', 'car', 'park', 'in', 'this', 'area', 'and', 'from', 'the', 'car', 'park', 'we', '’', 'll', 'enter', 'the', 'building', 'through', 'a', 'waiting', 'area', '.', 'This', 'leads', 'us', 'to', 'the', 'first', 'query', 'I', 'have', 'because', 'there', 'was', 'some', 'discussion', 'about', 'whether', 'you', 'wanted', 'the', 'size', 'of', 'the', 'waiting', 'room', 'increased', '.', 'At', 'the', 'moment', 'it', '’', 's', 'exactly', 'on', 'brief', ',', 'but', 'it', 'does', 'look', 'kind', 'of', 'small', 'to', 'my', 'eye', 'in', 'relation', 'to', 'the', 'size', 'of', 'the', 'project', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "tokens = word_tokenize(sample_text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b202c14b-5d59-4fbc-91cc-1092dae6fcad",
   "metadata": {},
   "source": [
    "### Removing Punctuations\n",
    "Different approaches can be used to remove punctuations. A helpful way is to use another library called [string](https://docs.python.org/3/library/string.html), which contains a list of standard punctuations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86eae962-46e9-4f9b-8316-d16a34f64c52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Well', 'the', 'design', 'has', 'developed', 'a', 'wee', 'bit', 'since', 'you', 'saw', 'it', 'last', 'time', 'the', 'design', 'obviously', 'is', 'still', 'in', 'exactly', 'the', 'same', 'place', 'but', 'the', 'design', 'is', 'extended', 'to', 'actually', 'include', 'the', 'actual', 'cremator', 'facility', 'so', 'if', 'I', 'can', 'start', 'with', 'this', 'particular', 'drawing', 'you', 've', 'seen', 'a', 'version', 'of', 'this', 'drawing', 'before', 'Basically', 'we', 're', 'arriving', 'in', 'the', 'new', 'car', 'park', 'in', 'this', 'area', 'and', 'from', 'the', 'car', 'park', 'we', 'll', 'enter', 'the', 'building', 'through', 'a', 'waiting', 'area', 'This', 'leads', 'us', 'to', 'the', 'first', 'query', 'I', 'have', 'because', 'there', 'was', 'some', 'discussion', 'about', 'whether', 'you', 'wanted', 'the', 'size', 'of', 'the', 'waiting', 'room', 'increased', 'At', 'the', 'moment', 'it', 's', 'exactly', 'on', 'brief', 'but', 'it', 'does', 'look', 'kind', 'of', 'small', 'to', 'my', 'eye', 'in', 'relation', 'to', 'the', 'size', 'of', 'the', 'project']\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "punctuations = string.punctuation + '’'\n",
    "tokens_without_puncts = [word for word in tokens if word not in punctuations]\n",
    "print(tokens_without_puncts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee3d663-a1a8-4155-bee6-e7c33de23ef5",
   "metadata": {},
   "source": [
    "## Counting Words\n",
    "Almost subsequent processing is about counting words at some level. A simple way to get a count of words for us is to use another library called [collections](https://docs.python.org/3/library/collections.html), and a function called [Counter](https://docs.python.org/3/library/collections.html#collections.Counter) in the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5632f4a8-57ac-4c56-9798-6c2b4d7a4761",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "word_counts = Counter(tokens_without_puncts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abfa5132-3d00-4d76-997c-6b4316334eb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 14, 'in': 4, 'to': 4, 'of': 4, 'design': 3, 'a': 3, 'you': 3, 'it': 3, 'this': 3, 'is': 2, 'exactly': 2, 'but': 2, 'I': 2, 'drawing': 2, 'we': 2, 'car': 2, 'park': 2, 'area': 2, 'waiting': 2, 'size': 2, 'Well': 1, 'has': 1, 'developed': 1, 'wee': 1, 'bit': 1, 'since': 1, 'saw': 1, 'last': 1, 'time': 1, 'obviously': 1, 'still': 1, 'same': 1, 'place': 1, 'extended': 1, 'actually': 1, 'include': 1, 'actual': 1, 'cremator': 1, 'facility': 1, 'so': 1, 'if': 1, 'can': 1, 'start': 1, 'with': 1, 'particular': 1, 've': 1, 'seen': 1, 'version': 1, 'before': 1, 'Basically': 1, 're': 1, 'arriving': 1, 'new': 1, 'and': 1, 'from': 1, 'll': 1, 'enter': 1, 'building': 1, 'through': 1, 'This': 1, 'leads': 1, 'us': 1, 'first': 1, 'query': 1, 'have': 1, 'because': 1, 'there': 1, 'was': 1, 'some': 1, 'discussion': 1, 'about': 1, 'whether': 1, 'wanted': 1, 'room': 1, 'increased': 1, 'At': 1, 'moment': 1, 's': 1, 'on': 1, 'brief': 1, 'does': 1, 'look': 1, 'kind': 1, 'small': 1, 'my': 1, 'eye': 1, 'relation': 1, 'project': 1})\n"
     ]
    }
   ],
   "source": [
    "print(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a7218f-8268-44f8-902a-879beb925ec6",
   "metadata": {},
   "source": [
    "## Letter case\n",
    "Looking at the list of non-repeating words in the sample text, we can see that capitalised letters are treated differently.\n",
    "We may or may not want this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "608cb938-19f3-4927-a17a-bbde22460cb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['At', 'Basically', 'I', 'This', 'Well', 'a', 'about', 'actual', 'actually', 'and', 'area', 'arriving', 'because', 'before', 'bit', 'brief', 'building', 'but', 'can', 'car', 'cremator', 'design', 'developed', 'discussion', 'does', 'drawing', 'enter', 'exactly', 'extended', 'eye', 'facility', 'first', 'from', 'has', 'have', 'if', 'in', 'include', 'increased', 'is', 'it', 'kind', 'last', 'leads', 'll', 'look', 'moment', 'my', 'new', 'obviously', 'of', 'on', 'park', 'particular', 'place', 'project', 'query', 're', 'relation', 'room', 's', 'same', 'saw', 'seen', 'since', 'size', 'small', 'so', 'some', 'start', 'still', 'the', 'there', 'this', 'through', 'time', 'to', 'us', 've', 'version', 'waiting', 'wanted', 'was', 'we', 'wee', 'whether', 'with', 'you']\n"
     ]
    }
   ],
   "source": [
    "non_repeating_words = word_counts.keys()\n",
    "print(sorted(non_repeating_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e753e9-eabc-41df-a661-269b41cb9808",
   "metadata": {},
   "source": [
    "### Converting all text to lowercase\n",
    "\n",
    "We simply use the `.lower()` method to convert all text to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2b5bcbf-f6a6-4e3f-9963-9fa23575fb43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'the': 14, 'in': 4, 'to': 4, 'this': 4, 'of': 4, 'design': 3, 'a': 3, 'you': 3, 'it': 3, 'is': 2, 'exactly': 2, 'but': 2, 'i': 2, 'drawing': 2, 'we': 2, 'car': 2, 'park': 2, 'area': 2, 'waiting': 2, 'size': 2, 'well': 1, 'has': 1, 'developed': 1, 'wee': 1, 'bit': 1, 'since': 1, 'saw': 1, 'last': 1, 'time': 1, 'obviously': 1, 'still': 1, 'same': 1, 'place': 1, 'extended': 1, 'actually': 1, 'include': 1, 'actual': 1, 'cremator': 1, 'facility': 1, 'so': 1, 'if': 1, 'can': 1, 'start': 1, 'with': 1, 'particular': 1, 've': 1, 'seen': 1, 'version': 1, 'before': 1, 'basically': 1, 're': 1, 'arriving': 1, 'new': 1, 'and': 1, 'from': 1, 'll': 1, 'enter': 1, 'building': 1, 'through': 1, 'leads': 1, 'us': 1, 'first': 1, 'query': 1, 'have': 1, 'because': 1, 'there': 1, 'was': 1, 'some': 1, 'discussion': 1, 'about': 1, 'whether': 1, 'wanted': 1, 'room': 1, 'increased': 1, 'at': 1, 'moment': 1, 's': 1, 'on': 1, 'brief': 1, 'does': 1, 'look': 1, 'kind': 1, 'small': 1, 'my': 1, 'eye': 1, 'relation': 1, 'project': 1})\n"
     ]
    }
   ],
   "source": [
    "lowercase_tokens = [word.lower() for word in tokens_without_puncts]\n",
    "word_counts_lowercase = Counter(lowercase_tokens)\n",
    "print(word_counts_lowercase)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04417bce-ee96-4298-aea6-847f041858d3",
   "metadata": {},
   "source": [
    "## Stemming and Lemmatization\n",
    "Note how `actual` and `actually` are treated separately. This may be necessary, or not, depending on the requirements of the analysis. If the base form of the word is to be obtained, we either have to \"stem\" the word (remove suffixes) or \"lemmatize\" the word (convert to base form).\n",
    "\n",
    "### Stemming\n",
    "\n",
    "This is the simple form where a set of rules can be used to remove inflection from the words. This may or may not work, as you can see from the below two examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "44fac450-6004-4b22-8ac7-08335a7b640a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discussion : discuss\n",
      "went : went\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem.porter import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    "print(\"discussion :\", stemmer.stem(\"discussion\"))\n",
    "print(\"went :\", stemmer.stem(\"went\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d00997f3-c8ef-4265-988a-66f4bb9627ae",
   "metadata": {},
   "source": [
    "Applying this approach to our word list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ed716ed2-1f36-4940-bd76-0e25dcbfb8c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['well', 'the', 'design', 'ha', 'develop', 'a', 'wee', 'bit', 'sinc', 'you', 'saw', 'it', 'last', 'time', 'obvious', 'is', 'still', 'in', 'exactli', 'same', 'place', 'but', 'extend', 'to', 'actual', 'includ', 'actual', 'cremat', 'facil', 'so', 'if', 'i', 'can', 'start', 'with', 'thi', 'particular', 'draw', 've', 'seen', 'version', 'of', 'befor', 'basic', 'we', 're', 'arriv', 'new', 'car', 'park', 'area', 'and', 'from', 'll', 'enter', 'build', 'through', 'wait', 'lead', 'us', 'first', 'queri', 'have', 'becaus', 'there', 'wa', 'some', 'discuss', 'about', 'whether', 'want', 'size', 'room', 'increas', 'at', 'moment', 's', 'on', 'brief', 'doe', 'look', 'kind', 'small', 'my', 'eye', 'relat', 'project']\n"
     ]
    }
   ],
   "source": [
    "stems = [stemmer.stem(word) for word in word_counts_lowercase]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc8dfe9-bf88-40e9-a2e0-234031d91d60",
   "metadata": {},
   "source": [
    "### Lemmatization\n",
    "\n",
    "This is a slightly more sophisticated version, where grammatical considerations are used to determine the base form of the word. For this, we also need to label the word with its appropriate part of speech."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62a23ebf-04ad-47cb-964f-82077c0420c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discussion : discussion\n",
      "went : go\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "\n",
    "lemmatize = WordNetLemmatizer().lemmatize\n",
    "print(\"discussion :\", lemmatize(\"discussion\", pos=\"n\"))\n",
    "print(\"went :\", lemmatize('went', pos='v'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb8e27af-dc85-4fe0-879f-4f2b0ae3dafb",
   "metadata": {},
   "source": [
    "Applying it to our list..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "805bc250-3f8e-491d-a29d-832649eeae9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['well', 'the', 'design', 'have', 'develop', 'a', 'wee', 'bit', 'since', 'you', 'saw', 'it', 'last', 'time', 'the', 'design', 'obviously', 'be', 'still', 'in', 'exactly', 'the', 'same', 'place', 'but', 'the', 'design', 'be', 'extend', 'to', 'actually', 'include', 'the', 'actual', 'cremator', 'facility', 'so', 'if', 'i', 'can', 'start', 'with', 'this', 'particular', 'drawing', 'you', 've', 'see', 'a', 'version', 'of', 'this', 'draw', 'before', 'basically', 'we', 're', 'arrive', 'in', 'the', 'new', 'car', 'park', 'in', 'this', 'area', 'and', 'from', 'the', 'car', 'park', 'we', 'll', 'enter', 'the', 'building', 'through', 'a', 'wait', 'area', 'this', 'lead', 'u', 'to', 'the', 'first', 'query', 'i', 'have', 'because', 'there', 'be', 'some', 'discussion', 'about', 'whether', 'you', 'want', 'the', 'size', 'of', 'the', 'wait', 'room', 'increase', 'at', 'the', 'moment', 'it', 's', 'exactly', 'on', 'brief', 'but', 'it', 'do', 'look', 'kind', 'of', 'small', 'to', 'my', 'eye', 'in', 'relation', 'to', 'the', 'size', 'of', 'the', 'project']\n"
     ]
    }
   ],
   "source": [
    "tagged_tokens = pos_tag(lowercase_tokens)\n",
    "lemmas_list = []\n",
    "\n",
    "for tagged_word in tagged_tokens:\n",
    "    word, pos_tag = tagged_word\n",
    "    if pos_tag.startswith(\"V\") :\n",
    "        lemma = lemmatize(word, pos=\"v\")\n",
    "    elif pos_tag.startswith(\"R\") :\n",
    "        lemma = lemmatize(word, pos=\"r\")\n",
    "    elif pos_tag.startswith(\"N\") :\n",
    "        lemma = lemmatize(word, pos=\"n\")\n",
    "    elif pos_tag.startswith(\"J\") :\n",
    "        lemma = lemmatize(word, pos=\"a\")\n",
    "    else :\n",
    "        lemma = lemmatize(word)\n",
    "    lemmas_list.append(lemma)\n",
    "\n",
    "print(lemmas_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddcd91b-869d-4740-adc3-0a57738c1c00",
   "metadata": {},
   "source": [
    "What difference do you see between the two lists?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b58d10-2284-4088-b2b1-0c5d13ac4e9a",
   "metadata": {},
   "source": [
    "## Extracting n-grams from text\n",
    "\n",
    "For identifying commonly-used phrases in a given text, you need to capture all possible occurrences of word sequences of the length that interests you.\n",
    "\n",
    "### Bigrams\n",
    "If the sequence is of two words, it is called a bigram. There is a function in the NLTK library, which is called `bigrams`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c7790bd-5f75-46c9-82c4-3a6e6a07eeac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('well', 'the'), ('the', 'design'), ('design', 'has'), ('has', 'developed'), ('developed', 'a'), ('a', 'wee'), ('wee', 'bit'), ('bit', 'since'), ('since', 'you'), ('you', 'saw'), ('saw', 'it'), ('it', 'last'), ('last', 'time'), ('time', 'the'), ('the', 'design'), ('design', 'obviously'), ('obviously', 'is'), ('is', 'still'), ('still', 'in'), ('in', 'exactly'), ('exactly', 'the'), ('the', 'same'), ('same', 'place'), ('place', 'but'), ('but', 'the'), ('the', 'design'), ('design', 'is'), ('is', 'extended'), ('extended', 'to'), ('to', 'actually'), ('actually', 'include'), ('include', 'the'), ('the', 'actual'), ('actual', 'cremator'), ('cremator', 'facility'), ('facility', 'so'), ('so', 'if'), ('if', 'i'), ('i', 'can'), ('can', 'start'), ('start', 'with'), ('with', 'this'), ('this', 'particular'), ('particular', 'drawing'), ('drawing', 'you'), ('you', 've'), ('ve', 'seen'), ('seen', 'a'), ('a', 'version'), ('version', 'of'), ('of', 'this'), ('this', 'drawing'), ('drawing', 'before'), ('before', 'basically'), ('basically', 'we'), ('we', 're'), ('re', 'arriving'), ('arriving', 'in'), ('in', 'the'), ('the', 'new'), ('new', 'car'), ('car', 'park'), ('park', 'in'), ('in', 'this'), ('this', 'area'), ('area', 'and'), ('and', 'from'), ('from', 'the'), ('the', 'car'), ('car', 'park'), ('park', 'we'), ('we', 'll'), ('ll', 'enter'), ('enter', 'the'), ('the', 'building'), ('building', 'through'), ('through', 'a'), ('a', 'waiting'), ('waiting', 'area'), ('area', 'this'), ('this', 'leads'), ('leads', 'us'), ('us', 'to'), ('to', 'the'), ('the', 'first'), ('first', 'query'), ('query', 'i'), ('i', 'have'), ('have', 'because'), ('because', 'there'), ('there', 'was'), ('was', 'some'), ('some', 'discussion'), ('discussion', 'about'), ('about', 'whether'), ('whether', 'you'), ('you', 'wanted'), ('wanted', 'the'), ('the', 'size'), ('size', 'of'), ('of', 'the'), ('the', 'waiting'), ('waiting', 'room'), ('room', 'increased'), ('increased', 'at'), ('at', 'the'), ('the', 'moment'), ('moment', 'it'), ('it', 's'), ('s', 'exactly'), ('exactly', 'on'), ('on', 'brief'), ('brief', 'but'), ('but', 'it'), ('it', 'does'), ('does', 'look'), ('look', 'kind'), ('kind', 'of'), ('of', 'small'), ('small', 'to'), ('to', 'my'), ('my', 'eye'), ('eye', 'in'), ('in', 'relation'), ('relation', 'to'), ('to', 'the'), ('the', 'size'), ('size', 'of'), ('of', 'the'), ('the', 'project')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import bigrams\n",
    "bigrams_from_text = list(bigrams(lowercase_tokens))\n",
    "print(bigrams_from_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea8b7969-1017-4f8c-9468-bc2eb87cb546",
   "metadata": {},
   "source": [
    "### Counting bigrams\n",
    "It is then a matter of simply counting the number of occurrences, similar to what we had done with words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f72bcd58-3820-4ce8-bd65-b71f8e543bde",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('the', 'design'): 3, ('car', 'park'): 2, ('to', 'the'): 2, ('the', 'size'): 2, ('size', 'of'): 2, ('of', 'the'): 2, ('well', 'the'): 1, ('design', 'has'): 1, ('has', 'developed'): 1, ('developed', 'a'): 1, ('a', 'wee'): 1, ('wee', 'bit'): 1, ('bit', 'since'): 1, ('since', 'you'): 1, ('you', 'saw'): 1, ('saw', 'it'): 1, ('it', 'last'): 1, ('last', 'time'): 1, ('time', 'the'): 1, ('design', 'obviously'): 1, ('obviously', 'is'): 1, ('is', 'still'): 1, ('still', 'in'): 1, ('in', 'exactly'): 1, ('exactly', 'the'): 1, ('the', 'same'): 1, ('same', 'place'): 1, ('place', 'but'): 1, ('but', 'the'): 1, ('design', 'is'): 1, ('is', 'extended'): 1, ('extended', 'to'): 1, ('to', 'actually'): 1, ('actually', 'include'): 1, ('include', 'the'): 1, ('the', 'actual'): 1, ('actual', 'cremator'): 1, ('cremator', 'facility'): 1, ('facility', 'so'): 1, ('so', 'if'): 1, ('if', 'i'): 1, ('i', 'can'): 1, ('can', 'start'): 1, ('start', 'with'): 1, ('with', 'this'): 1, ('this', 'particular'): 1, ('particular', 'drawing'): 1, ('drawing', 'you'): 1, ('you', 've'): 1, ('ve', 'seen'): 1, ('seen', 'a'): 1, ('a', 'version'): 1, ('version', 'of'): 1, ('of', 'this'): 1, ('this', 'drawing'): 1, ('drawing', 'before'): 1, ('before', 'basically'): 1, ('basically', 'we'): 1, ('we', 're'): 1, ('re', 'arriving'): 1, ('arriving', 'in'): 1, ('in', 'the'): 1, ('the', 'new'): 1, ('new', 'car'): 1, ('park', 'in'): 1, ('in', 'this'): 1, ('this', 'area'): 1, ('area', 'and'): 1, ('and', 'from'): 1, ('from', 'the'): 1, ('the', 'car'): 1, ('park', 'we'): 1, ('we', 'll'): 1, ('ll', 'enter'): 1, ('enter', 'the'): 1, ('the', 'building'): 1, ('building', 'through'): 1, ('through', 'a'): 1, ('a', 'waiting'): 1, ('waiting', 'area'): 1, ('area', 'this'): 1, ('this', 'leads'): 1, ('leads', 'us'): 1, ('us', 'to'): 1, ('the', 'first'): 1, ('first', 'query'): 1, ('query', 'i'): 1, ('i', 'have'): 1, ('have', 'because'): 1, ('because', 'there'): 1, ('there', 'was'): 1, ('was', 'some'): 1, ('some', 'discussion'): 1, ('discussion', 'about'): 1, ('about', 'whether'): 1, ('whether', 'you'): 1, ('you', 'wanted'): 1, ('wanted', 'the'): 1, ('the', 'waiting'): 1, ('waiting', 'room'): 1, ('room', 'increased'): 1, ('increased', 'at'): 1, ('at', 'the'): 1, ('the', 'moment'): 1, ('moment', 'it'): 1, ('it', 's'): 1, ('s', 'exactly'): 1, ('exactly', 'on'): 1, ('on', 'brief'): 1, ('brief', 'but'): 1, ('but', 'it'): 1, ('it', 'does'): 1, ('does', 'look'): 1, ('look', 'kind'): 1, ('kind', 'of'): 1, ('of', 'small'): 1, ('small', 'to'): 1, ('to', 'my'): 1, ('my', 'eye'): 1, ('eye', 'in'): 1, ('in', 'relation'): 1, ('relation', 'to'): 1, ('the', 'project'): 1})\n"
     ]
    }
   ],
   "source": [
    "bigram_counts = Counter(bigrams_from_text)\n",
    "print(bigram_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a3aae1-bb5c-480b-964e-b8ac438c28e1",
   "metadata": {},
   "source": [
    "### Generalizing to n-grams\n",
    "We use a similar utility called `n-grams` to generalize this idea to words of any length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "68bf9daf-a998-420f-b6a6-ae29afbb66bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({('the', 'size', 'of'): 2, ('size', 'of', 'the'): 2, ('well', 'the', 'design'): 1, ('the', 'design', 'has'): 1, ('design', 'has', 'developed'): 1, ('has', 'developed', 'a'): 1, ('developed', 'a', 'wee'): 1, ('a', 'wee', 'bit'): 1, ('wee', 'bit', 'since'): 1, ('bit', 'since', 'you'): 1, ('since', 'you', 'saw'): 1, ('you', 'saw', 'it'): 1, ('saw', 'it', 'last'): 1, ('it', 'last', 'time'): 1, ('last', 'time', 'the'): 1, ('time', 'the', 'design'): 1, ('the', 'design', 'obviously'): 1, ('design', 'obviously', 'is'): 1, ('obviously', 'is', 'still'): 1, ('is', 'still', 'in'): 1, ('still', 'in', 'exactly'): 1, ('in', 'exactly', 'the'): 1, ('exactly', 'the', 'same'): 1, ('the', 'same', 'place'): 1, ('same', 'place', 'but'): 1, ('place', 'but', 'the'): 1, ('but', 'the', 'design'): 1, ('the', 'design', 'is'): 1, ('design', 'is', 'extended'): 1, ('is', 'extended', 'to'): 1, ('extended', 'to', 'actually'): 1, ('to', 'actually', 'include'): 1, ('actually', 'include', 'the'): 1, ('include', 'the', 'actual'): 1, ('the', 'actual', 'cremator'): 1, ('actual', 'cremator', 'facility'): 1, ('cremator', 'facility', 'so'): 1, ('facility', 'so', 'if'): 1, ('so', 'if', 'i'): 1, ('if', 'i', 'can'): 1, ('i', 'can', 'start'): 1, ('can', 'start', 'with'): 1, ('start', 'with', 'this'): 1, ('with', 'this', 'particular'): 1, ('this', 'particular', 'drawing'): 1, ('particular', 'drawing', 'you'): 1, ('drawing', 'you', 've'): 1, ('you', 've', 'seen'): 1, ('ve', 'seen', 'a'): 1, ('seen', 'a', 'version'): 1, ('a', 'version', 'of'): 1, ('version', 'of', 'this'): 1, ('of', 'this', 'drawing'): 1, ('this', 'drawing', 'before'): 1, ('drawing', 'before', 'basically'): 1, ('before', 'basically', 'we'): 1, ('basically', 'we', 're'): 1, ('we', 're', 'arriving'): 1, ('re', 'arriving', 'in'): 1, ('arriving', 'in', 'the'): 1, ('in', 'the', 'new'): 1, ('the', 'new', 'car'): 1, ('new', 'car', 'park'): 1, ('car', 'park', 'in'): 1, ('park', 'in', 'this'): 1, ('in', 'this', 'area'): 1, ('this', 'area', 'and'): 1, ('area', 'and', 'from'): 1, ('and', 'from', 'the'): 1, ('from', 'the', 'car'): 1, ('the', 'car', 'park'): 1, ('car', 'park', 'we'): 1, ('park', 'we', 'll'): 1, ('we', 'll', 'enter'): 1, ('ll', 'enter', 'the'): 1, ('enter', 'the', 'building'): 1, ('the', 'building', 'through'): 1, ('building', 'through', 'a'): 1, ('through', 'a', 'waiting'): 1, ('a', 'waiting', 'area'): 1, ('waiting', 'area', 'this'): 1, ('area', 'this', 'leads'): 1, ('this', 'leads', 'us'): 1, ('leads', 'us', 'to'): 1, ('us', 'to', 'the'): 1, ('to', 'the', 'first'): 1, ('the', 'first', 'query'): 1, ('first', 'query', 'i'): 1, ('query', 'i', 'have'): 1, ('i', 'have', 'because'): 1, ('have', 'because', 'there'): 1, ('because', 'there', 'was'): 1, ('there', 'was', 'some'): 1, ('was', 'some', 'discussion'): 1, ('some', 'discussion', 'about'): 1, ('discussion', 'about', 'whether'): 1, ('about', 'whether', 'you'): 1, ('whether', 'you', 'wanted'): 1, ('you', 'wanted', 'the'): 1, ('wanted', 'the', 'size'): 1, ('of', 'the', 'waiting'): 1, ('the', 'waiting', 'room'): 1, ('waiting', 'room', 'increased'): 1, ('room', 'increased', 'at'): 1, ('increased', 'at', 'the'): 1, ('at', 'the', 'moment'): 1, ('the', 'moment', 'it'): 1, ('moment', 'it', 's'): 1, ('it', 's', 'exactly'): 1, ('s', 'exactly', 'on'): 1, ('exactly', 'on', 'brief'): 1, ('on', 'brief', 'but'): 1, ('brief', 'but', 'it'): 1, ('but', 'it', 'does'): 1, ('it', 'does', 'look'): 1, ('does', 'look', 'kind'): 1, ('look', 'kind', 'of'): 1, ('kind', 'of', 'small'): 1, ('of', 'small', 'to'): 1, ('small', 'to', 'my'): 1, ('to', 'my', 'eye'): 1, ('my', 'eye', 'in'): 1, ('eye', 'in', 'relation'): 1, ('in', 'relation', 'to'): 1, ('relation', 'to', 'the'): 1, ('to', 'the', 'size'): 1, ('of', 'the', 'project'): 1})\n"
     ]
    }
   ],
   "source": [
    "from nltk.util import ngrams\n",
    "trigrams_from_text = list(ngrams(lowercase_tokens, 3))\n",
    "trigram_counts = Counter(trigrams_from_text)\n",
    "print(trigram_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee7056b7-4c7d-4e9d-8c86-16a49a8391e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['draw', 'draw', 'draw', 'drawer', 'draw']\n"
     ]
    }
   ],
   "source": [
    "test_words = ['draw', 'drawing', 'drew', 'drawer', 'drawn']\n",
    "lemmas_test = [lemmatize(w, 'v') for w in test_words]\n",
    "print(lemmas_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a04507d8-4ca4-4867-bb90-8fcf92fd1973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['draw', 'draw', 'drew', 'drawer', 'drawn']\n"
     ]
    }
   ],
   "source": [
    "stems_test = [stemmer.stem(w) for w in test_words]\n",
    "print(stems_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "88539f08-4f05-4db3-9cf2-a21ed3e74ea8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"that'll\", 'through', 'doesn', \"needn't\", 'before', 'again', 're', 'nor', 'can', \"should've\", 'needn', 'wasn', 'yourself', 'had', 'i', 'to', 'down', 'a', 'hasn', 'haven', 'than', 'not', 'it', 'doing', 'off', 'this', 'didn', 'yours', 'should', 'o', 'be', 'itself', 'whom', \"isn't\", 'such', 'but', 'my', 'what', 'now', 'mightn', 'up', 'some', 's', 'in', \"haven't\", 'hers', 'below', 'there', 'other', 'just', 'these', 'shan', 'am', 'weren', \"weren't\", 'will', \"wasn't\", 'aren', 'where', 'shouldn', 'so', 'do', 'them', 'most', 't', 'or', \"you're\", \"you'll\", 'has', 'by', \"she's\", 'and', \"mustn't\", \"doesn't\", 'they', 'having', 'ma', 'here', 'until', \"aren't\", 'as', \"you've\", 'own', 'were', 'is', 'we', 'all', 'if', 'under', \"don't\", 'are', 'his', 'm', 'd', \"hasn't\", \"mightn't\", 'you', 'against', 'that', 'at', 'me', \"didn't\", 'same', 'its', 'of', 'does', 'their', 'into', 'from', 'an', 'more', 'which', 'for', 'himself', \"wouldn't\", 'no', 'themselves', \"you'd\", 'ours', 'after', \"it's\", 'her', 'our', \"shan't\", 'on', 'isn', 'being', 've', 'll', 'with', 'during', \"hadn't\", 'won', 'herself', 'because', 'once', 'he', 'the', 'him', 'was', 'very', 'mustn', 'hadn', \"couldn't\", 'both', 'theirs', 'between', 'out', 'how', 'above', 'she', \"shouldn't\", 'those', 'while', 'did', \"won't\", 'been', 'too', 'y', 'wouldn', 'don', 'then', 'ourselves', 'myself', 'who', 'when', 'about', 'few', 'ain', 'over', 'couldn', 'why', 'yourselves', 'each', 'your', 'have', 'further', 'any', 'only'}\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "# nltk.download('stopwords') # uncomment and run this line the first time you run this code.\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "603b9070-5c68-46aa-8920-e5aebee5b417",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'design': 3, 'exactly': 2, 'drawing': 2, 'car': 2, 'park': 2, 'area': 2, 'waiting': 2, 'size': 2, 'well': 1, 'developed': 1, 'wee': 1, 'bit': 1, 'since': 1, 'saw': 1, 'last': 1, 'time': 1, 'obviously': 1, 'still': 1, 'place': 1, 'extended': 1, 'actually': 1, 'include': 1, 'actual': 1, 'cremator': 1, 'facility': 1, 'start': 1, 'particular': 1, 'seen': 1, 'version': 1, 'basically': 1, 'arriving': 1, 'new': 1, 'enter': 1, 'building': 1, 'leads': 1, 'us': 1, 'first': 1, 'query': 1, 'discussion': 1, 'whether': 1, 'wanted': 1, 'room': 1, 'increased': 1, 'moment': 1, 'brief': 1, 'look': 1, 'kind': 1, 'small': 1, 'eye': 1, 'relation': 1, 'project': 1})\n"
     ]
    }
   ],
   "source": [
    "lowercase_tokens_nostop = [word for word in lowercase_tokens if not word in stop_words]\n",
    "word_counts_lowercase_nostop = Counter(lowercase_tokens_nostop)\n",
    "print(word_counts_lowercase_nostop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69078140-82d9-4093-ae25-89677e3b73ba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
